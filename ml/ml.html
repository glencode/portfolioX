<!DOCTYPE HTML>
<html>
	
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Projectworks</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="Projectworks" />
	<meta name="keywords" content="github,projects,machine learning, datascience" />
	<meta name="author" content="Glenn Gitamo" />
        
    <!--quora icon--> 
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	
	<link href="../fonts.googleapis.com/css6cda.css?family=Quicksand:300,400,500,700" rel="stylesheet">
	<link href="../fonts.googleapis.com/css5c05.css?family=Playfair+Display:400,400i,700" rel="stylesheet">
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Flaticons  -->
	<link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="css/owl.theme.default.min.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">
	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
        
	</head>
            
	<body>
	<div id="colorlib-page">
		<div class="container-wrap">
		<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a> 
		<aside id="colorlib-aside" role="complementary" class="border js-fullheight">
			<div class="text-center">
				<div class="author-img" style="background-image: url(images/mllogo.jpeg);"></div>
				<!-- <h1 id="colorlib-logo"><a href="ht.github.io/">Glenn Gitamo</a></h1> -->
				<em>"If data is the new oil, then machine learning is the engine that will drive 
					the future."</em><p>-Dave Waters</p>
				<p><strong>Jump to:</strong></p>

				<nav id="colorlib-main-menu" role="navigation" class="navbar">
						<!-- <div id="navbar" class="collapse"> -->
						<ul>
							<li><a href="#cifar10">CIFAR-10 OBJECT RECOGNITION</a></li>
							<li><a href="#conservision">conser-vision</a></li>
							<li><a href="#playstore">GOOGLE PLAY STORE APPS AND REVIEWS ANALYSIS</a></li>
							<li><a href="#bigmart">BIG MART SALES PREDICTION</a></li>
							<li><a href="#calories">CALORIES BURNT PREDICTION APP</a></li>
							<li><a href="#carprice">USED CAR PRICE PREDICTION</a></li>
							<li><a href="#creditapproval">CREDIT CARD APPROVAL SYSTEM</a></li>
							<li><a href="#creditfraud">CREDIT CARD FRAUD DETECTION</a></li>
							<li><a href="#customerseg">CUSTOMER SEGMENTATION ANALYSIS</a></li>
							<li><a href="#medinsurance">MEDICAL COST INSURANCE PREDICTION</a></li>
							<li><a href="#moviereco">MOVIE RECOMMENDATION SYSTEM</a></li>
							<li><a href="#titanic">TITANIC SURVIVAL WEBAPP</a></li>
							<li><a href="#winetasting">WINE QUALITY PREDICTION</a></li>
							<li><a href="#multidisease">MULTIPLE DISEASE DETECTION SYSTEM</a></li>
							<li><a href="#netflix">NETFLIX ANALYSIS</a></li>
							<li><a href="#nobel">NOBEL PRIZE ANALYSIS</a></li>

						</ul>
						<!-- </div> -->
				</nav>
				
			</div>
			
			
		</aside>

		<div id="colorlib-main">
		
	<section class="colorlib-about" data-section="about">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm animate-box" data-animate-effect="fadeInLeft">
								<div class="col-md-12">
									<div class="about-desc">
										<h2>Machine Learning projects</h2>
                                        <hr>
                                        <h3 class="colourlib-heading" id="cifar10">CIFAR-10 object recognition </h3>
                                        <h4><strong>Object Detection(Computer Vision)</strong></h4>
                                        <img src="images/cifar1009.jpeg">
                                        <p>The objective of this project was to develop an object detection model using the CIFAR-10 dataset. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The classes include airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The goal was to train a deep learning model to accurately detect and classify objects in the CIFAR-10 images.</p>
										<img src="images/Screenshot (161).png">
										<p>Despite the low quality of the input image, the trained model was able to successfully detect underlying patterns, demonstrating its ability to learn and generalize from the training data</p>
										<img src="images/Screenshot (160).png">
										<br><br>
                                        <p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/cifar10objectrecognition">here</a></p>
										
										<p><strong>Used Technologies include:</strong> Python, OpenCV, Pandas, NumPy, Tensorflow, Jupyter</p>
                                        <br>
                                        <br><br>

			<section class="colorlib-about" data-section="Conser-vision">
                                        <h3 id="conservision" class="colourlib-heading">Conser-vision</h3>
                                        <h4><strong>Object Detection and Image Classification(Computer Vision)</strong></h4>
                                        <!-- <img src="images/cifar1009.jpeg"> -->
                                        <p> In this challenge which was a competition from drivendata.org, my goal was to classify the species that appear in camera trap images collected by their research partners at the Wild Chimpanzee Foundation and the Max Planck Institute for Evolutionary Anthropology.

											My job was to build a model that can help researchers predict whether an image contains one of seven types of species.
											
											For this challenge, I was provided with images, along with a few attributes of each image that might be helpful in setting up your training and testing sets.
											
											Each record in the dataset corresponds to a single image captured by a camera trap. Each record had one .jpg file associated with it.
											<br><br>
											
											Here is an example of a single image in the dataset - ZJ000048, which captures a monkey in the act of looking cute:</p>
											
											<img src="images/monkeycute.jpg">
											<br><br>
										<p>Here is the associated metadata for this image provided in train_features.csv:</p>
										<img src="images/monkeycutemeta.png">
										<br><br>
										<p>Here is an example of a blank image (ZJ000144), where there are no animals detected:</p>
										<img src="images/blankanimal.jpg">
										<br><br>
										<p>These are its accompanying characteristics in train_features.csv:</p>
										<img src="images/blankanimalmeta.png">
										<br><br>
										<p>Finally, here is an image from test_features (ZJ016556) and unlike the previous two images, which were a part of the training set, there is no test_labels.csv file to look up for this one! My model had to do the work:</p>
										<img src="images/testanimal.jpg">
										<br><br>
										<p>Here is the information i had from test_features.csv:</p>
										<img src="images/testanimalmeta.png">
										<br><br>
										<br><br>
										<p>The animals were sometimes maybe close or far from the camera, in the sun or in the shadows, or facing toward or away from the lens, among other variations. There are also differences in the color of the images, the weather conditions it was taken, and the type of camera.
											In order to teach the model to generalize well despite these differences, it was helpful to perform a preprocessing step called image augmentation. Image augmentation involves transforming the training set in multiple ways—rotating, distorting in color or sharpness, zooming in or out, are a few examples. These manipulations of the image helped the model make correct predictions in contexts it has not been exposed to before.
										</p>
										<br><br>
										
									    <p>There were eight possible labels for every image. If the image does not contain any animals, it is labeled as blank. Otherwise it had to be labeled as containing one of the seven species groups included in the dataset:
											<ol>
												<li>antelope_duiker</li>
												<li>bird</li>
												<li>civet_genet</li>
												<li>hog</li>
												<li>leopard</li>
												<li>monkey_prosimian</li>
												<li>rodent</li>
											  </ol>
											  
										</p>
                                        <br><br>
										
										<p>My submitted file contained nine columns:
											<ul>
												<li>id (string): unique identifier for each row from test_features.csv</li>
												<li>antelope_duiker, bird, blank, civet_genet, hog,leopard, monkey_prosimian, rodent (float, target variables): there is a column for each of the eight possible classes containing the probability that the image is of that class.</li>
										    </ul>
										</p>
										<br><br>
										
										<p>The predictions for the target variables are float probabilities that range between 0.0 and 1.0. Each row adds up to 1, since the classes are mutually exclusive. My submitted file looks like this:</p>
										<img src="images/submission.png" width="90%" height="auto">
										<br><br>
										<br><br>
										<p>To measure my model's accuracy by looking at prediction error, I used a metric called log loss. This is an error metric, so a lower value is better (as opposed to an accuracy metric, where a higher value is better). My model achieves a nice score of 2.1960</p>
										<br><br>
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Conser-visionCL">here</a></p>
										
										<p><strong>Used Technologies include:</strong> Python, OpenCV, Pandas, Tensorflow, Jupyter, Colab, DrivenData</p>
                                        <br>
                                        <br><br>
			</section>
                                        <h3 class="colourlib-heading" id="playstore">Google Play Store apps and reviews Analysis</h3>
                                        <h4><strong>Data Wrangling(Data Science)</strong></h4>
                                        <p> Mobile apps are everywhere.
											They are easy to create and can be lucrative. 
											Because of these two factors, more and more apps are being developed. 
											In this project I did a comprehensive analysis of the Android app market by comparing 
											over ten thousand apps in Google Play across different categories to look for insights 
											in the data so as to devise strategies to drive growth and retention
										</p>										
											<img src="images/playlogo.jpeg">
											<br><br>
										<p>With more than 1 billion active users in 190 countries around the world, Google Play continues to be an important distribution platform to build a global audience. For businesses to get their apps in front of users, it's important to make them more quickly and easily discoverable on Google Play. To improve the overall search experience, Google has introduced the concept of grouping apps into categories.</p>
											<p>This brings up the following questions:</p>
											<ul>
											<li>Which category has the highest share of (active) apps in the market? </li>
											<li>Is any specific category dominating the market?</li>
											<li>Which categories have the fewest number of apps?</li>
											</ul>
											<p>On investigation, I see that there are <code>33</code> unique app categories present in the acquired dataset. <em>Family</em> and <em>Game</em> apps have the highest market prevalence. Interestingly, <em>Tools</em>, <em>Business</em> and <em>Medical</em> apps are also at the top.</p>
											<br><br>
											<img src="images/plotlycatdist.png" width="90%" height="auto">
											<br><br>
											<br><br>
											<p>After having witnessed the market share for each category of apps,
											  I saw how all these apps perform on an average. App ratings (on a scale of 1 to 5) 
											  impact the discoverability, conversion of apps as well as the company's overall brand 
											  image. Ratings are a key performance indicator of an app.</p>
											<p>From my research, I found that the average volume of ratings across all app categories is 
												<code>4.17</code>.
											  The histogram plot below is skewed to the left indicating that the majority 
											  of the apps are highly rated with only a few exceptions in the low-rated apps.</p>
											  <img src="images/histofrating.png" width="90%" height="auto">
										
										<br><br>
										<br><br>
										<p>Next I examined app size and app price. 
										   For size, if the mobile app is too large, it may be difficult and/or expensive 
										   for users to download. Lengthy download times could turn users off before 
										   they even experience your mobile app. Plus, each user's device has a finite 
										   amount of disk space. For price, some users expect their apps to be free or 
										   inexpensive. These problems compound if the developing world is part of your 
										   target market; especially due to internet speeds, earning power and exchange rates.</p>
										<p>How can we effectively come up with strategies to size and price our app?</p>
										<ul>
										<li>Does the size of an app affect its rating? </li>
										<li>Do users really care about system-heavy apps or do they prefer light-weighted apps? </li>
										<li>Does the price of an app affect its rating? </li>
										<li>Do users always prefer free apps over paid apps?</li>
										</ul>
										<p>I found that the majority of top rated apps (rating over 4) range from 2 MB to 20 MB.
											I also found that the vast majority of apps price themselves under $10.
											<img src="images/sizerating.png">
											<img src="images/pricerating.png">
										
										</p>
										<br><br>
										<p>So then comes an important part. How are companies and developers supposed to make ends meet? 
											What monetization strategies can companies use to maximize profit? 
											The costs of apps are largely based on features, complexity, and platform.</p>
										<p>There are many factors to consider when selecting the right pricing strategy for your mobile app. It is important to consider the willingness of your customer to pay for your app. A wrong price could break the deal before the download even happens. Potential customers could be turned off by what they perceive to be a shocking cost, or they might delete an app they’ve downloaded after receiving too many ads or simply not getting their money's worth.</p>
										<p>Different categories demand different price ranges. Some apps that are simple and used daily,
											 like the calculator app, should probably be kept free. However, it would make sense to charge 
											 for a highly-specialized medical app that diagnoses diabetic patients. 
											 Below, we see that <em>Medical and Family</em> apps are the most expensive. 
											 Some medical apps extend even up to $80! All game apps are reasonably priced below $20.</p>
										<img src="images/downloadpercost.png">
										<img src="images/downloadpercost2.png" width=90% height="auto">
										<br><br>
										<p>On having a good look it looks like a bunch of the really expensive apps are "junk" apps.
											 That is, apps that don't really have a purpose. Some app developer may create an app called 
											 <em>I Am Rich Premium</em> or <em>most expensive app </em> 
											 just for a joke or to test their app development skills.
											  Some developers even do this with malicious intent and try to make money
											   by hoping people accidentally click purchase on their app in the store.</p>
										<p>I filtered out these junk apps and visualized with our new data.
										<img src="images/downloadpercostfiltered.png" width=90% height="auto">
										<br><br>
										<p>For apps in the Play Store today, there are five types of pricing strategies: 
											free, freemium, paid, paymium, and subscription. I focused on free and paid apps only. 
											Some characteristics of free apps are:</p>
										<ul>
										<li>Free to download.</li>
										<li>Main source of income often comes from advertisements.</li>
										<li>Often created by companies that have other products and the app serves as an 
											extension of those products.</li>
										<li>Can serve as a tool for customer retention, communication, and customer service.</li>
										</ul>
										<p>Some characteristics of paid apps are:</p>
										<ul>
										<li>Users are asked to pay once for the app to download and use it.</li>
										<li>The user can't really get a feel for the app before buying it.</li>
										</ul>
										<p>Are paid apps installed as much as free apps? It turns out that paid apps have a relatively
											 lower number of installs than free apps, though the difference is not as large as 
											 I would have expected!</p>
										<img src="images/paidvsfree.png" width=90% height="auto">
										<br><br>
										<br><br>
										<p>Finally by plotting sentiment polarity scores of user reviews for paid and free apps,
											 I observed that free apps receive a lot of harsh comments, as indicated by the outliers
											  on the negative y-axis. Reviews for paid apps appear never to be extremely negative. 
											  This indicate something about app quality, i.e., paid apps being of higher quality than 
											  free apps on average. The median polarity score for paid apps is a little higher than free 
											  apps, thereby syncing with the previous observation.</p>
										<p>I analyzed over ten thousand apps from the Google Play Store and can use the findings to
											 inform decisions should one ever wish to create an app themselves.
										</p>
										<img src="images/sentimentpolarity.png" width=90% height="auto">
										<br><br>
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/AndroidMarketAnalysis">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python, NumPy, Pandas, Plotly, Jupyter, Seaborn, Matplotlib</p>
                                        <br>
                                        <br><br>                                 
										
										<h3 class="colourlib-heading" id="bigmart">Big Mart Sales Prediction</h3>
                                        <h4><strong>Predictive Modeling(Regression)</strong></h4>
                                        <p> In this project, I worked on a dataset that contains information about sales of products
											 in a supermarket chain. The goal is to build a model to predict the sales of the products
											  in the future. To achieve this goal,
											 I used various techniques of data preprocessing, exploratory data analysis, and machine learning.
											 The dataset used in this project was obtained from Kaggle.
											  It contains information about 8,523 products and their sales in 10 different stores. 
											 The dataset has 12 features, including both numerical and categorical variables.
										</p>										
											<!-- <img src="images/playlogo.jpeg"> -->
											<br><br>
										<p>EXPLORATORY DATA ANALYSIS:
											I started the project by exploring the dataset using various plots and statistics.
											 First, I checked for missing values and filled them using mean and mode. 
											 Then, I analyzed the distribution of the numerical variables, including Item_Weight, 
											 Item_Visibility, Item_MRP, and Item_Outlet_Sales. I also plotted the count of the 
											 establishment year of the stores. For the categorical variables, 
											I plotted the count of each category, including Item_Fat_Content, Item_Type, and Outlet_Size.</p>
											<img src="images/bigmart1.png" width=90% height="auto">
											<img src="images/bigmart2.png" width=90% height="auto">
											<img src="images/bigmart3.png" width=90% height="auto">
											<img src="images/bigmart4.png" width=90% height="auto">
											<img src="images/bigmart5.png" width=90% height="auto">
											<img src="images/bigmart6.png" width=90% height="auto">
											<img src="images/bigmart7.png" width=90% height="auto">
											<img src="images/bigmart8.png" width=90% height="auto">
											<img src="images/bigmart9.png" width=90% height="auto">
											<br><br>
											<br><br>
											<p>DATA PREPROCESSING:
												After exploring the dataset, I preprocessed the data for machine learning. 
												I first replaced the inconsistent categories in Item_Fat_Content with Low Fat and Regular.
												 Then, I used LabelEncoder to encode the categorical variables. Finally, 
												 I split the data into train and test datasets.
												 <br><br>
												MODEL BUILDING AND EVALUATION:
												I used XGBoost, a powerful gradient boosting algorithm, to build the model. 
												I trained the model on the train dataset and evaluated it using R-squared 
												and Root Mean Squared Error (RMSE) on both the train and test datasets. 
												The model achieved an R-squared score of 0.64 and RMSE of 1,074.11 on the train dataset, 
												and an R-squared score of 0.57 and RMSE of 1,204.03 on the test dataset.
												<br><br>
												CONCLUSION:
												In this project, I used various techniques of data preprocessing, exploratory data analysis,
												 and machine learning to build a model to predict the sales of products in a supermarket chain. 
												 The model achieved a moderate performance, with an R-squared score of 0.57 and RMSE of 1,204.03 
												 on the test dataset. Future work can include trying different machine
												 learning algorithms and feature engineering techniques to improve the performance of the model.
												</p>
											
										
										<!-- <img src="images/downloadpercostfiltered.png" width=90% height="auto"> -->
										<br><br>
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/BigMartSales">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Jupyter Notebook,
											NumPy,
											Pandas,
											Matplotlib,
											Seaborn,
											Scikit-learn,
											XGBoost</p>
                                        <br>
                                        <br><br> 
										
										<h3 class="colourlib-heading" id="calories">Calories Burnt Prediction App</h3>
                                        <h4><strong>Predictive Modeling(Regression)</strong></h4>
                                        <p> This project aims to predict the calories burnt by an individual during exercise 
											using XGBoost regression. The dataset used in this project includes information on the 
											individual's age, gender, weight, height, and exercise details. The model 
											is trained on the given dataset and is used to predict the number of calories burnt during exercise.

											The dataset used in this project is a combination of two datasets, 'calories.csv' and 
											'exercise.csv'. Both datasets are available in the 'data' directory. The 'calories.csv'
											 dataset contains information on the individual's age, gender, height, weight, and calories 
											 burnt during exercise. The 'exercise.csv' dataset contains
											 information on the individual's exercise details, including exercise type, duration, 
											 and heart rate.
										</p>										
											<!-- <img src="images/playlogo.jpeg"> -->
											<br><br>
										<p>DATA PREPARATION:
											The 'User_ID' column is dropped from the 'calories.csv' dataset, and the two datasets 
											are merged to create a single dataset named 'the_dataset'. The 'Gender' column is converted 
											to binary values (0 for male, 1 for female), and the correlation between the variables 
											is calculated.
										</p>
												<p>MODEL BUILDING AND EVALUATION:
												The dataset is split into training and testing sets, with 80% of the data used for training 
												and the remaining 20% used for testing. The XGBoost regression model is trained on the training 
												data, and the predictions are made on the testing data. 
												The performance of the model is evaluated using the R-squared error and mean absolute error 
												(MAE)
												 <br><br>
												MODEL DEPLOYMENT:
												The trained model is saved in a pickle file named 'finalized_model.sav'. A web application is
												 created using Flask, HTML, CSS, and JavaScript that takes 
												input from the user and predicts the number of calories burnt during exercise using 
												the trained model.
												<br><br>
												CONCLUSION:
												In this project, we have successfully built a model that can predict the number of calories burnt 
												during exercise based on the given 
												input parameters. The model can be further improved by using more data and tuning the 
												hyperparameters.
												</p>
											
										
										<img src="images/calories_B.png" width="auto" height="auto">
										<br><br>
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Calories_Burnt_App">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											NumPy,
											Pandas,
											Seaborn,
											Scikit-learn,
											XGBoost,
											Flask,
											HTML/CSS/JavaScript</p>
                                        <br>
                                        <br><br>

										<h3 class="colourlib-heading" id="carprice"> Used Car Price Prediction</h3>
                                        <h4><strong>Predictive Modeling(Regression)</strong></h4>
                                        <p> The goal of this project is to build a machine learning model to predict the selling price 
											of used cars based on various features. The dataset used for this project contains information 
											about used cars, including the car name, 
											the year of purchase, the selling price, the fuel type, the transmission type, and more.
										</p>										
											<!-- <img src="images/playlogo.jpeg"> -->
										<p>Data Preprocessing:
											The dataset was loaded into a Pandas DataFrame. Categorical data was encoded with integer values. 
											The data was then split into training and testing sets using the train_test_split function from 
											Scikit-learn.
											<br><br>
											Model Training:
											Two regression models were trained and evaluated: Linear Regression and Lasso Regression.
											 For each model, the R-squared error was calculated for both the training and testing datasets.
											<br><br>
											Model Evaluation:
											The actual and predicted prices for both the training and testing datasets were visualized using
											 scatter plots. The models were evaluated based on their R-squared error, and the best model was 
											 selected.
											<br><br>
											Results:
											
											The best model was found to be Lasso Regression, which achieved an R-squared error of 0.76 
											on the testing dataset. This indicates that the model can explain 76% of the variance in the 
											selling price of used cars. The model can be used to predict the selling 
											price of a used car based on its features.
											<br><br>
											Conclusion:
											
											This project demonstrated how to build a machine learning model to predict the selling price of 
											used cars. The Lasso Regression model achieved the best performance, with an R-squared error of 
											0.76 on the testing dataset. This project can be expanded by using more advanced machine learning 
											techniques, or by incorporating additional features into the model.
										</p>
																	
										<img src="images/car_price0.jpg" width="auto" height="auto">
										<br><br>
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Car_Price_Prediction">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											NumPy,
											Pandas,
											Seaborn,
											Scikit-learn,
											Lasso, Ridge,
											Jupyter Notebook
										</p>
                                        <br>
                                        <br><br>

										<h3 class="colourlib-heading" id="creditapproval"> Credit Card Approval System</h3>
                                        <h4><strong>Predictive Modeling(Classification)</strong></h4>
                                        <p> Commercial banks receive <em>a lot</em> of applications for credit cards. Many of them get 
											rejected for many reasons, like high loan balances, low income levels, or too many inquiries on 
											an individual's credit report, for example. Manually analyzing these applications is mundane, 
											error-prone, and time-consuming (and time is money!). Luckily, this task can be 
											automated with the power of machine learning and pretty much every commercial bank does so nowadays.
										</p>										
										<img src="images/credit_card0.jpg" width=100% height=75%>
										<br><br>
										<p>I used the <a href="http://archive.ics.uci.edu/ml/datasets/credit+approval">
										Credit Card Approval dataset</a> from the UCI Machine Learning Repository.
									    </p>
                                        <img src="images/credit_card1.png">
										<br><br>
										<p><em>Exploratory Data Analysis:</em><br>On inspection I could see that the dataset had a mixture of 
											numerical and non-numerical features. 
											This was fixed with some preprocessing, but before even doing that, i wanted to learn 
											about the dataset a bit more to see if there are other dataset issues that needed to be fixed.
										</p>
										<p>I split the data into train set and test set to prepare the data for two different phases 
											of machine learning modeling: training and testing.
											 Ideally, no information from the test data should be used to preprocess the training data or
											  should be used to direct the training process of a machine learning model. Hence, I first split
											   the data and then preprocessed it.
										</p>
										<p>Also, features like <code>DriversLicense</code> and <code>ZipCode</code> were not as important 
											as the other features in the dataset for predicting credit card approvals so I dropped them to 
											design the model with the best set of features
										</p>
										<p><em>Data Preprocessing:</em><br>After handling the missing values and ensuring that all data was now numeric,
											 I scaled the feature values to a uniform range.
										</p>
										<p>To understand what these scaled values mean in the real world.I used <code>CreditScore</code>
											 as an example. The credit score of a person is their creditworthiness based on their credit
											  history. The higher this number, the more financially trustworthy a person is considered to be. 
											  So, a <code>CreditScore</code> 
											of 1 is the highest since I was rescaling all the values to the range of 0-1.
										</p>
										<p><em>Model Building and Evaluation:</em><br> The dataset contained more instances that correspond to 
											"Denied" status than instances corresponding to "Approved" status. Specifically, out of 690 
											instances, there are 383 (55.5%) applications 
											that got denied and 307 (44.5%) applications that got approved. 
										</p>
										<p>This gave me a benchmark. A good machine learning model should be able to accurately predict the 
											status of the applications with respect to these statistics.
										</p>
										<p>A good question to ask was: <em>are the features that affect the credit card approval decision 
											process correlated with each other?</em> Because of this correlation, I took advantage of the 
											fact that generalized linear models perform well in these cases to start building the machine 
											learning modeling with a Logistic Regression model (a generalized linear model).
										</p>
										<p>It was now time to see how the model performs. </p>
										<p>I now evaluated the model on the test set with respect to the metric; classification accuracy </a>and
											 also took a look the model's confusion matrix</a>. In the case of predicting credit card 
											 applications, it is important to see if the machine learning model is equally capable of 
											 predicting approved and denied status, in line with the frequency of these labels in our original
											  dataset. If the model would not be performing well in this aspect, then it might end up
											   approving the application that should have been approved. 
											The confusion matrix helped me to view the model's performance from these aspects.  
										</p>
										<img src="images/credit_card2.png">
										<p>The model was pretty good and in fact, it was able to yield an accuracy score of 100%!.</p>
										<p>For the confusion matrix, the first element of the first row of the confusion matrix 
											denotes the true negatives meaning the number of negative instances (denied applications) 
											predicted by the model correctly. And the last element of the second row of the confusion matrix 
											denotes the true positives meaning the number of positive instances (approved applications) 
											predicted by the model correctly.
										</p>
										<p>But if I didn't get a perfect score what was to be done? I could perform a grid search of the 
											model parameters to improve the model's ability to predict credit card approvals.
										</p>
										<!-- <br><br> -->
										<img src="images/credit_card3.jpg">
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Credit_Approval_System">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Pandas,
											Scikit-learn,
											XGBoost,
											Jupyter Notebook
										</p>
										
                                        <br>
                                        <br><br>

										<h3 class="colourlib-heading" id="creditfraud"> Credit Card Fraud Detection</h3>
                                        <h4><strong>Predictive Modeling(Classification)</strong></h4>
										<img src="images/credit_fraud0.jpeg" width="auto" height=75%>
										<br><br>
                                        <p> The purpose of this project was to build a classification model to detect fraudulent credit card 
											transactions. Credit card fraud is a common problem that results in significant financial losses 
											for individuals and businesses. By accurately identifying fraudulent transactions, 
											financial institutions can take appropriate measures to prevent or minimize these losses
										</p>
										<p>The dataset used for this project is the Credit Card Fraud Detection dataset, which is available on 
											Kaggle. The dataset contains transaction data from European credit cardholders over a period of 
											two days in September 2013. The dataset 
											has 284,807 transactions, out of which 492 are fraudulent, making it a highly imbalanced dataset.
										</p>										
										
										<img src="images/credit_fraud1.png">
										<br><br>
										<p><em>Data Exploration and Preprocessing:</em><br>
											On exploration I found that there was no correlation between the 'Amount' feature and the 'Class' 
											feature. 
											However, transactions with higher amounts tended to be legitimate transactions.
											<br>I also did not find missing values in the dataset. The 'Time' column was dropped, 
											and the 'Amount' column was scaled using the StandardScaler from the scikit-learn library.
									    </p>
										<p><em>Model Selection and Evaluation:</em><br>
											Logistic Regression was selected as the classification model for this project due to its 
											simplicity and efficiency in handling binary classification problems.<br>
											
											The dataset was split into training and testing sets, and the training set was used to train the 
											model. The accuracy metric was used to evaluate the model's performance on both the training and 
											testing sets. Hyperparameter tuning 
											was not performed due to the limited number of hyperparameters in the Logistic Regression model.
										</p>
										<p><em>Results:</em><br>The Logistic Regression model achieved an accuracy of 94.4% on the training 
											set and 93.8% on the testing set. The model was able to detect 85% of fraudulent transactions in 
											the testing set, with a precision of 89%.<br>

											The model's strengths include its simplicity and efficiency in handling binary classification 
											problems. However, it may not perform well on datasets with more complex relationships between 
											features.
										</p>
										
										<img src="images/credit_fraud2.png">
										<br><br>
										<p><em>Conclusion and Future Work:</em><br>In conclusion, the Logistic Regression model was able to 
											accurately identify fraudulent transactions in the Credit Card Fraud Detection dataset. Future 
											work could involve exploring more complex models, such as Random Forest or Gradient Boosting, to 
											improve the model's performance on more complex datasets. Additionally, collecting more data 
											on fraudulent transactions could help to improve the model's ability to detect such transactions.
										</p>
										<br>
                                        <img src="images/credit_fraud3.jpg" width="900">
										<br><br>
										
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Credit_Card_Fraud_Detection">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Pandas,
											Scikit-learn,
											Matplotlib, Seaborn, Logistic Regression,
											Jupyter Notebook
										</p>
										
                                        <br>
                                        <br><br>

										<h3 class="colourlib-heading" id="customerseg"> Customer Segmentation Analysis</h3>
                                        <h4><strong>Unsupervised Learning(Clustering)</strong></h4>
										<img src="images/customer_seg0.jpeg">
										<br><br>
                                        <p> The purpose of this project was to cluster customers into different segments based on their annual
											 income and spending score in order to identify different groups 
											of customers and create targeted marketing strategies to improve customer satisfaction and revenue.
										</p>
										<p>The dataset used in this analysis was the "Mall Customer Segmentation Data"
										    which contained information about the annual income and spending score of customers in a mall.
										</p>
										<img src="images/customer_seg1.png" width="1200" height="auto">										
										<p>
											After loading the dataset, the annual 
											income and spending score columns I selected and stored them in a NumPy array for further analysis.
										</p>
										
										<br><br>
										<p><em>Exploratory Data Analysis:</em><br>
											I used the elbow method to determine the optimal number of clusters to use for the K-Means 
											clustering algorithm. A plot of the within cluster sum of squares (WCSS) was generated against the
											 number of clusters. The optimum number of clusters I found was 5.

											The K-Means clustering algorithm was then trained on the dataset with 5 clusters. The resulting cluster 
											labels were stored in a vector.

											<!-- I also used a scatter plot to visualize the clusters and their centroids -->
									    </p>
										<img src="images/customer_seg2.png">
										<br><br>
										<p><em>Model Selection and Evaluation:</em><br>
											I chose the K-Means clustering algorithm for this analysis because it is a 
											simple and efficient unsupervised learning algorithm that is widely used for customer 
											segmentation.<br>
											
											The evaluation process involved 
											visually inspecting the scatter plot to identify any patterns, trends or insights in the data.
										</p>
										<br>
										<img src="images/customer_seg3.png" width="1200">
										<p><em>Results:</em><br>The scatter plot shows 5 distinct clusters of customers 
											based on their annual income and spending score. The clusters can be described as follows:
											<ul><li>Cluster 1: High annual income, high spending score</li>
												<li>Cluster 2: Average annual income, average spending score</li>
												<li>Cluster 3: Low annual income, high spending score</li>
												<li>Cluster 4: Low annual income, low spending score</li>
												<li>Cluster 5: High annual income, low spending score</li></ul>
										</p>
										<br><br>
										<p><em>Conclusion and Future Work:</em><br>In conclusion, I successfully clustered 
											customers into different segments based on their annual income and spending score. This can help 
											the mall management to create targeted marketing strategies for each customer segment, thereby 
											improving customer satisfaction and revenue.

											Future work includes incorporating more features into the analysis. The analysis can also be extended to 
											include demographic data to gain deeper insights into the behavior of different customer segments.
										</p>
										<br>
                                        <img src="images/customer_seg4.jpeg" width="800" height="400">
										<br><br>
										
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Customer_Segmentation_Analysis">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Pandas,
											Scikit-learn,
											Matplotlib, Seaborn, Kmeans,
											Jupyter Notebook
										</p>
                                       <!-- <br> -->
										<br><br>	
										<h3 class="colourlib-heading" id="medinsurance"> Medical Cost Insurance Prediction</h3>
                                        <h4><strong>Predictive Modeling(Regression)</strong></h4>
										<img src="images/insurance1.jpeg">
										<br><br>
                                        <p> In this project, the goal was to develop a machine learning model that can accurately predict 
											health insurance charges
											 based on various factors such as age, gender, BMI, number of children, smoking habit, and region. 
										</p>
										<p>The data was collected from the US region through various channels like surveys, online forms, and
											 real-world scenarios
										</p>
																			
										<p>
											The dataset did not contain any missing information. I also checked for outliers and 
											decided to keep them in the data as they could potentially provide valuable information for the 
											model. Then I scaled the 
											numerical features to ensure that they were on the same scale and easier for the model to learn 
											from.
										</p>
										
										<br><br>
										<p><em>Exploratory Data Analysis:</em><br>
											I created plots to visualize the data so as to understand the distribution of the data and identify 
											any patterns or correlations between the features.

											<!-- I also used a scatter plot to visualize the clusters and their centroids -->
									    </p>
										<img src="images/insuranceage.png" width=95%>
										<img src="images/insurancesex.png" width=95%>
										<img src="images/insurancebmi.png" width=95%>
										<img src="images/insurancechildren.png" width=95%>
										<img src="images/insurancesmoker.png" width=95%>
										<img src="images/insuranceregion.png" width=95%>
										<img src="images/insurancecharge.png" width=95%>
										<br><br>
										<p><em>Model Selection and Evaluation:</em><br>
											I also performed feature engineering to extract additional features from the data, such as the 
											smoker status and BMI category.<br>

											I now split the dataset into training and testing sets for training and 
											evaluation of the model. 
											I used mean absolute error (MAE) and R-squared metrics to evaluate the performance of the model.
										</p>
										<br>
										<!-- <img src="images/customer_seg3.png" width="1200"> -->
										<p><em>Results:</em><br>The model achieved an MAE of 2,452 and an R-squared score of 
											0.78, which indicated that the model was a good fit with the data. I also observed that 
											the smoking status and BMI category were the most significant factors in determining medical 
											charges.
										</p>
										<br><br>
										<p><em>Conclusion and Future Work:</em><br>In conclusion, I developed a machine learning model that 
											can accurately predict health insurance charges
											 based on various factors such as age, gender, BMI, number of children, smoking habit, and 
											 region.

											However, there is still room for improvement, and in future studies I could explore additional 
											factors 
											such as occupation, education, or lifestyle, which may also affect medical charges and hence model
											performance.
										</p>
										<br>
                                        <img src="images/insurance2.jpeg" width="800" height="400">
										<br><br>
										
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Medical_Insurance_Prediction">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Pandas, Numpy
											Scikit-learn,
											Matplotlib, Seaborn, Linear Regression,
											Jupyter Notebook
										</p>
                                        <br><br>
										<h3 class="colourlib-heading" id="moviereco"> Movie Recommendation System</h3>
                                        <h4><strong>NLP(Content-based Filtering)</strong></h4>
										<img src="images/movie_reco0.jpeg">
										<br><br>
                                        <p> With the vast amount of data available, it is crucial to use 
											machine learning techniques to help users make informed decisions about what movies they should 
											watch next. In this project, I built a movie recommendation system using a content-based 
											filtering technique. I created a feature matrix that represents each
											 movie and computed the cosine similarity between the feature vectors to find the most similar 
											 movies. 
										</p>
										<p>The data used in this project was obtained from Kaggle. The dataset contains information 
											about 45,000 movies, including metadata such as cast, crew, plot summaries, and user ratings.
										</p>
										<br><br>
										<p><em>Data Preprocessing:</em><br>
											The dataset contained some missing values, which I handled by replacing them with an empty string. 
											I then 
											selected a subset of features to use for the recommendation system, including keywords, cast, 
											genres, director, vote_average, and overview. I combine these 
											features into a single string and use a TF-IDF vectorizer to convert the text into numeric 
											vectors.
									    </p>
										<br>
										<p><em>Model Implementation:</em><br>
											I compute the cosine similarity between the feature vectors and visualize the 
											resulting 
											similarity matrix. This is the matrix I use 
											to recommend movies to users by finding the movies with the highest cosine similarity to a given 
											movie.<br>
										</p>
										<br>
										<!-- <img src="images/customer_seg3.png" width="1200"> -->
										<p><em>Results:</em><br>The 
											system performs well, being able to provide a list of the top 20 most similar movies to an input
											 movie. (see sample output below)
										</p>
										<img src="images/movie_recotop201.png">
										<img src="images/movie_recotop202.png">
										<br><br>
										<p><em>Future Work:</em><br>Further research can explore the 
											use of hybrid recommendation systems to provide more accurate and personalized recommendations.
										</p>
										<br>
                                        <img src="images/movie_reco2.jpeg" width="750" height="400">
										<br><br>
										
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Movie_Recommendation_System">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Pandas, Numpy, 
											Scikit-learn,
											Matplotlib, Seaborn, NLP(Content-based Filtering),
											Jupyter Notebook
										</p>
                                        <br><br>
										<h3 class="colourlib-heading" id="titanic"> Titanic Survival WebApp</h3>
                                        <h4><strong>Predictive Modeling(Classification)</strong></h4>
										<img src="images/titanic1.jpeg">
										<br><br>
                                        <p> This project aims to create a system able to predict whether a passenger on the
											 Titanic survived or not, given some input features such as the passenger's age, sex, class, 
											and fare. .
										</p>
										<p>The data for this project was sourced from Kaggle. The data 
											consists of information about 
											passengers on the Titanic, including their age, sex, class, fare, and whether or not they survived.
										</p>
										<br><br>
										<p><em>Exploratory Data Analysis:</em><br>
											I explored the dataset to gain insights into the data and 
											understand the relationship between the features and the target variable. I used visualizations 
											such as bar plots, histograms, and box plots to visualize the data.<br>
										<img src="images/titanicisna1.png">
										<img src="images/titanicsurvivedc.png">
										<img src="images/titanicsex.png">
										<img src="images/titanicage.png">
										<img src="images/titanicpclass.png">
										<img src="images/titanicfare.png">
										<img src="images/titanicsibsp.png">
										<img src="images/titanicpclage.png">
										<img src="images/titanicisna2.png">
										
									    </p>
										<p><em>Data Cleaning and Preprocessing:</em><br>
											I did some cleaning and preprocessing on the dataset to ensure that the data was in a 
											suitable format for use in a machine learning model. Techniques used for cleaning and 
											preprocessing included handling missing values, 
											dropping irrelevant columns, and converting categorical data to binary data using one-hot 
											encoding.
									    </p>
										<br>
										<p><em>Model Selection and Evaluation:</em><br>
											I chose a neural network model for this project, using TensorFlow's Keras API. Feature 
											engineering techniques were used to create new features from existing ones, such as creating age 
											categories and fare categories.

											The model was trained using the training set and evaluated on the testing set. The Scikit-learn 
											library 
											was used to split the dataset into training and testing sets, as well as to scale the features.
											<br>
										</p>
										<br>
										<!-- <img src="images/customer_seg3.png" width="1200"> -->
										<p><em>Results:</em><br>
											The model was able to achieve an accuracy of approximately 80% on the test set. The limitations 
											of the system included
											 the fact that the dataset was limited in size and did not include information about all 
											 passengers on the Titanic.

											Interpretation of the results and their implications showed that factors such as age, sex, and 
											class were strong predictors of survival on the Titanic.
										</p>
										<img src="images/titanic_history.png">
										<br><br>
										<p>
											I created a Flask web application that allows users to input values for the features of a 
											passenger and get a prediction for whether that passenger survived or not.(see sample input/output below)
										</p>
										<img src="images/titanic_app1.png">
										<img src="images/titanic_app2.png">
										
										<br><br><br>
										<p><em>Conclusion and Future Work:</em><br>
											In conclusion, the Titanic project was a successful application of machine learning techniques 
											to a real-world problem. The main findings showed that factors such as age, sex, and class were 
											strong predictors of survival on the Titanic. Future research could explore 
											other factors that may have influenced survival rates, as well as larger and more diverse 
											datasets.
										</p>
										<br>
                                        <img src="images/titanic2.jpeg" width="750" height="400">
										<br><br>
										
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Titanic_Ult1">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Pandas, Numpy, 
											Scikit-learn,
											Matplotlib, Seaborn, TensorFlow, Keras, Flask,
											Jupyter Notebook
										</p>
                                        <br><br><br>
										<h3 class="colourlib-heading" id="winetasting"> Wine Quality Prediction</h3>
                                        <h4><strong>Predictive Modeling(Classification)</strong></h4>
										<img src="images/winequality0.jpeg">
										<br><br>
                                        <p> This project aims to create a system able to evaluate whether a red wine is of good or bad quality
											. This will be of great resource to a company seeking to start making new brands of wine ensuring
											efficient, honest and even more accurate assessment of a wine. 
											The created model is able to make the distinction between good and bad red wine, given some input 
											features
											such as the fixed acidity, volatile acidity, citric acid, residual sugar, 
											chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol.
											 
										</p>
										
										<br><br>
										<p><em>Exploratory Data Analysis:</em><br>
											I explored the dataset to gain insights into the data and 
											understand the relationship between the features and the target variable and I used some plots
											to visualize. <br>
										<img src="images/qualitycount.png">
										<img src="images/qualityvvolatile.png">
										<img src="images/qualityvcitric.png">
										<img src="images/qualityheatmap.png">
										</p>
										<p><em>Data Cleaning and Preprocessing:</em><br>
											The dataset was fairly clean but  I had to binarize the quality feature since it was categorical 
											going from 3 to 8, 8 indicating the wine was of great quality, but I binarized since the 
											aim was to know whether a wine was good or bad.
																					
										</p>
										<br>
										<p><em>Model Selection and Evaluation:</em><br>
											I split the data into training and testing sets and chose a random forest classifier model to train
											on the training set.<br>
											I evaluated the model using the accuracy_score from scikit-learn and got a good accuracy of 93%
											on the test set.<br>
											I also passed some new instances to the model and it was able to correctly identify what wines 
											were good and which were bad. 

										<br>
										</p>
										<br>
										
										<p><em>Conclusion and Future Work:</em><br>
											In conclusion, the Wines project was a successful application of machine learning techniques 
											to a real-world problem. The system was able to learn how each of the factors it was trained on 
											affected the quality of the wine. The system could now be deployed to be used by someone or 
											a company seeking to make a new brand of wine or maybe just like the plain old art of wine tasting. 
											<br>Future research could explore incorporating newer ingredients of wine into the model since 
											the world of wine is ever-growing as people discover great ways to make the best wines.
											<br><br>Cheers! 
										</p>
										<br>
                                        <img src="images/winequality1.jpeg" width="750" height="400">
										<br><br>
										
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Wine_Quality_Prediction">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Pandas, Numpy, 
											Scikit-learn,
											Matplotlib, Seaborn, RandomForestClassifier, 
											Jupyter Notebook
										</p>
                                        <br><br><br>
										<h3 class="colourlib-heading" id="multidisease"> Multiple Disease Detection System</h3>
                                        <h4><strong>Predictive Modeling(Classification)</strong></h4>
										<img src="images/multiple_diseases0.jpeg">
										<br><br>
                                        <p> This project aims to create a system able to detect whether a person has a certain disease 
											such as
											Cancer, Heart Disease, Diabetes or Parkinsons. This will assist the medics to quickly make 
											accurate assessments and probably save lives.
											The app is user-friendly hence any user with access is able to just input the required features
											and the results are given on display. 
										</p>							
										<br><br>
										<p><em>Data Preparation and Preprocessing:</em><br>
											Each of the diseases prediction utilizes its own model in the backend so firstly, 
											all the required trained models are loaded so that the app would be able to function as
											expected.
										<br>
											Then the user is now presented with the home page where they are able to choose a disease they
											want to make their assessment on if they wish to.																											
										</p>
										<img src="images/diseasesselect.png" width="80%">
										<br><br><br>
										<p>
											The user will then be prompted to fill out the form with the details respective to the disease.
											If the given data is valid, it is now encoded, scaled, and transformed and is then passed to the
											model for prediction. 
										</p>
										<img src="images/diseasesdiabetes.png" width="80%">
										<br><br><br>
										<p><em>Results:</em><br>
											The model returns its prediction and the user is able to view a displayed message output
											outlining whether or not they have the disease.

										<br>
										</p>
										<img src="images/diseasesresults.png" width="80%">
										<br><br><br>
										
										<p><em>Conclusion and Future Work:</em><br>
											In conclusion, the Disease detection project was a successful application of machine learning
											techniques as the deployed app is able to assist in the real world of health care.
											The underlying models were trained on the various features that constitute a particular 
											disease hence the system is able to generalize to the new given instances and 
											could now be deployed to be used by a medical user or 
											a healthcare facility seeking to make time-sensitive but accurate assessments so as to take the 
											actions necessary. <br>
											Future developments could see more diseases integrated into the app but due to computational power 
											the application is constrained for now.
										</p>
										<br>
                                        <img src="images/multiple_diseases1.jpeg" width="750" height="400">
										<br><br>
										
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/multiple_disease_prediction">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Pandas, Numpy, 
											Scikit-learn,
											Matplotlib, Seaborn, Tensorflow, Keras, Streamlit, Support Vector Machines, Logistic Regression,
											Jupyter Notebook
										</p>
										<br><br><br>
										<h3 class="colourlib-heading" id="netflix"> Netflix Analysis</h3>
                                        <h4><strong>Data Science(Data Analysis)</strong></h4>
										<img src="images/netflix0.png" width="400">
										<br><br>
                                        <p>Netflix! What started in 1997 as a DVD rental service has since exploded into the largest 
											entertainment/media company by market capitalization, boasting over 
											200 million subscribers as of January 2021. Given the large number of movies and series available
											 on the platform, it is a perfect opportunity to engage Data Science in the entertainment 
											 industry.
										</p>
										<p> My Data Science colleague firmly believed that the average duration of movies has been
											 declining. As evidence of this, they 
											provided me with the following information. For the years from 2011 to 2020, the average movie
											 durations are 103, 101, 99, 100, 100, 95, 95, 96, 93, and 90, respectively.
										</p>							
										<br>
										<p><em>A visual inspection of this data:</em><br>
											Ofcourse we took a Pythonic approach to be sure of this revelation, so we created a 
											dictionary on the data and loaded this dictionary to a Pandas dataframe.
											We wanted to follow up on their assertion that movie 
											lengths have been decreasing over time so a great place to start was visualization of the 
											data. We want to follow up on our friend's assertion that movie lengths have been decreasing 
											over time. A great place to start will be a visualization of the data. Given that the data 
											is continuous, a line plot would be a good choice, with the dates represented along the x-axis
											and the average length in minutes along the y-axis. <br>
											This would allow us to easily spot any trends in movie durations.
										</p>
										<br>
										<img src="images/netflixdf1.png">
										<img src="images/netflixline1.png">
										<br><br>
										<p><em>Data Preparation:</em><br>
											Well, it sure looked like there is something to the idea that movie lengths have 
												decreased over the past ten years! But equipped only with so little aggregations, 
												we were limited in the further explorations we can perform. There are a few questions
												 about this trend that we were unable to answer, including:
											<ol>
											<li>What does this trend look like over a longer period of time?</li>
											<li>Is this explainable by something like the genre of entertainment?</li>
											</ol>
												<p>We took it upon ourselves to collect more data from the web and created a more robust 
												and adequate dataframe with almost 8000 data points and more features to work with.</p>
												(see the first few rows below) 																								
										</p>
										<img src="images/netflixdf2.png" width="850">
										<br><br>
										
										<p><em>Data Preprocessing:</em><br>
											We had to do some cleaning as data from the web tends to be untidy and performed our feature 
											selection which finally had us which a dataframe of five features including<code>title</code>, 
											<code>country</code>, <code>genre</code>, <code>release_year</code> and, <code>duration</code>.
											(first few rows below)
										<br>
										</p>
										<img src="images/netflixdf3.png">
										<br>
										
										<p><em>A visualization of our new data:</em><br>
											This time, since we were no longer working with aggregates but instead with individual movies,
											a line plot was no longer a good choice for our data, so we created 
											a scatter plot of the year of release on the x-axis and the movie duration on the y-axis.
										</p>
										<br>
										<img src="images/netflixscatter1.png">
										<br><br>
										<p><em>Exploring for insights:</em><br>
											We could now see that, while newer movies are overrepresented on the platform, many short
											movies have been released in the past two decades.<br>
											Upon further inspection, something else we noticed was that some of these films were under an 
											hour long! We decided to filter our DataFrame for movies with the Duration 
											feature for entries under 60 minutes and look at the genres.
										    This gave us some insight into what is dragging down the average and found that many of the 
											films that are under 60 minutes fall into genres such as "Children", "Stand-Up", and
											"Documentaries". This is a logical result, as these types of films are probably
											often shorter than 90-minute Hollywood blockbusters.
										</p>
										<p>
											We made a colors list that we can pass to our scatter plot(to reference these particular genres),
											 which would allow us to visually 
											inspect whether these genres might be responsible for the decline in the average duration of 
											movies.
										</p>
										<img src="images/netflixscatter2.png">
										<br><br>
										<p><em>Results and Findings:</em><br>
											Well, as I had suspected, non-typical genres such as children's movies and 
											documentaries are all clustered around the bottom half of the plot.
										</p>
										<img src="images/netflixresults.png">
										<br>
                                        <img src="images/netflix1.jpeg" width="750" height="400">
										<br><br>
										
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Netflix_Analysis">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Pandas, Numpy, 
											Scikit-learn,
											Matplotlib,
											Jupyter Notebook
										</p>
										
										<br><br><br>
										<h3 class="colourlib-heading" id="nobel"> Nobel Prize Analysis</h3>
                                        <h4><strong>Data Science(Data Analysis)</strong></h4>
										<img src="images/nobel0.jpeg">
										<br><br>
                                        <p>The Nobel Prize is perhaps the world's most well-known scientific award. Except for the honor, 
											prestige, and substantial prize money the recipient also gets a gold medal showing Alfred Nobel 
											(1833 - 1896) who established the prize. Every year it's given to scientists and scholars in the 
											categories of chemistry, literature, physics, physiology or medicine, economics, and peace. The 
											first Nobel Prize was handed out in 1901, and at that time the Prize was very
											 Eurocentric and male-focused, but nowadays it's not biased in any way whatsoever. Surely. 
											 Right?
										</p>
										<p> Well, in this project I was set to find out! The Nobel Foundation made a dataset 
											available of all prize winners from the start of the prize, in 1901, to 2016
										</p>							
										<br>
										<p>Below is a high-level view of some of the data features and some aggregations:</p>
										<img src="images/nobeldf1.png" width="750">
										<img src="images/nobelvaluecounts.png">
										<br>
										<p><em>A visual inspection of this data:</em><br>
											Just a quick look at the dataset and already I could see that all of the winners in 1901 were
											guys that came from Europe. But that was back in 1901, looking at all winners 
											in the dataset, from 1901 to 2016, which sex and which country is the most commonly 
											represented?  <br>
											For country, I decided to the birth_country feature of the winner, as the organization_country 
											was missing values for all shared Nobel Prizes.
										</p>
										<p>
											After having a better look it was not so surprising to see that according to the data the most
											common Nobel laureate between 1901 and 2016 was a man born in the United States of America.
											in the United States of America.<br>
											But in 1901 all the winners were European. When did the USA start to dominate the Nobel Prize 
											charts? I had to visualize this dominance.
										</p>
										<br>
										<img src="images/nobelusadominance.png">
										<br><br>
										<p><em>A look at men's dominance:</em><br>
											So the USA became the dominating winner of the Nobel Prize first in the 1930s and had kept the 
											leading position ever since. But one group that was in the lead from the start, and never seems 
											to let go, is men. Maybe it shouldn't come as a shock that there is some imbalance between how 
											many male and female prize winners there are, but how significant is this imbalance? 
											<br>Is it better or worse within specific prize categories like physics, medicine, 
											literature, etc.?
										</p>
										<img src="images/nobelfemale11.png">
										<br><br>
										<p>From visualizing with a plot I could see some interesting trends and patterns. Overall the 
											imbalance is pretty large with physics, economics, and chemistry having the largest imbalance.
										    Medicine has a somewhat positive trend, and since the 1990s the literature prize is also now 
											more balanced. The big outlier was the peace prize during the 2010s.
										</p>
										<p>Aware of this imbalance, it was good to now ask Who was the first woman to receive a Nobel 
											Prize? And in what category?<em>Her name was Marie Curie and she got it in Physics in the
												year 1903 and more interesting was that she actually got the nobel prize more than once!
											</em>

										</p>
										<p><em>But anyway, just how old are you when you get a prize?:</em><br>
											I used another plot to visualize this to understand.
										<br>
										</p>
										<img src="images/nobelage1.png">
										<p>From the visualization, I saw that people use to be around 55 when they received the price, but 
											nowadays the average is closer to 65. 
											But there is a large spread in the laureates' ages, and while most are 50+, some are very
											young.<br>
											I saw that the density of points is much high nowadays than in the early 1900s -- nowadays 
											many more of the prizes are shared, and so there are many more winners. 
											I also saw that there was a disruption in awarded prizes around the Second World War 
											(1939 - 1945).
										</p>
										<br>
										<p>Ok but now I wanted to find out what were the age differences between prize categories so I took
											a look at the various trends.
										</p>
										<img src="images/nobelchemistry.png">
										<br><br>
										<p>More plots showed lots of exciting stuff going on! I could see that both winners of the 
											chemistry,
										    medicine, and physics prize have gotten older over time. The trend is strongest for physics: 
											the average age used to be below 50, and now it's almost 70. Literature and economics are more 
											stable. I also saw that economics is a newer category. But peace showed an opposite trend where
											winners are getting younger! 
										</p>
										<p>In the peace category there was also a winner around 2010 that seemed exceptionally young. This
											 begged the questions, who are the oldest and youngest people ever to have won a Nobel 
											 Prize?
										</p>
										<img src="images/nobelest.png">
										<br><br>
																			
                                        <img src="images/nobel1.jpeg" width="750" height="400">
										<br><br>
										
										
										<p>Full project code can be found on my GitHub repository <a href="https://github.com/glencode/Nobel_Analysis">here</a>.</p>
										<p><strong>Used Technologies include:</strong> Python,
											Pandas, Numpy, 
											Scikit-learn,
											Matplotlib, Seaborn,
											Jupyter Notebook
										</p>
									</div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section>


	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>
	<!-- Owl carousel -->
	<script src="js/owl.carousel.min.js"></script>
	<!-- Counters -->
	<script src="js/jquery.countTo.js"></script>
	
	
	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	</body>

</html>



